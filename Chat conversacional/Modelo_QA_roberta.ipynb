{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo conversacional usando Ollama - Llama 3.1\n",
    "\n",
    " - Se ha descargado el software Ollama: https://ollama.com/ con la finalidad de optimizar los recursos computacionales para el agente conversacional mediante LLM\n",
    " - Ademas, se ha descargado el modelo Llama 3.1 (4.7GB) para así no depender de API o dependencias externas y asegurar el funcionamiento.\n",
    " - La fuente principal del codigo implementado aqui proviene de: https://python.langchain.com/docs/how_to/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import  ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion sobre los articulos, esto emula lo que simula una variable que contendra la informacion de los articulos alojados en la BBDD\n",
    "info_articles = \"\"\"\n",
    "\n",
    "Título: Large Scale Enrichment and Statistical Cyber Characterization of Network Traffic\n",
    "\n",
    "Autores: Ivan Kawaminami, Arminda Estrada, Youssef Elsakkary, Hayden Jananthan, Aydın Buluç, Tim Davis, Daniel Grant, Michael Jones, Chad Meiners, Andrew Morris, Sandeep Pisharody, Jeremy Kepner\n",
    "Fecha: 7 de septiembre de 2022\n",
    "Resumen: Este estudio aborda el desafío de analizar grandes volúmenes de datos de red mediante la correlación cruzada de sensores de red, enriqueciendo cada evento con metadatos adicionales. Se emplean los marcos de análisis Python GraphBLAS y PyD4M para realizar análisis estadísticos anónimos y eficientes en conjuntos de datos de red de gran escala. \n",
    "\n",
    "Título: The Future is Meta: Metadata, Formats and Perspectives towards Interactive and Personalized AV Content\n",
    "\n",
    "Autores: Alexander Weller, Werner Bleisteiner, Christian Hufnagel, Michael Iber\n",
    "Fecha: 28 de julio de 2024\n",
    "Resumen: Este artículo explora cómo los metadatos están transformando la producción, distribución y consumo de contenido audiovisual interactivo y personalizado. Se discuten los requisitos y enfoques potenciales para integrar metadatos en infraestructuras existentes, destacando proyectos como Tailored Media, ARD Audiothek y ORPHEUS. \n",
    "Título: Discrete Analysis: Una revista basada en arXiv\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui es dondd \n",
    "template = \"\"\"\n",
    "Answer the question below in Spanish:\n",
    "\n",
    "Here is the article context:\n",
    "{info_articles}\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo llama3.1 que como se menciono en el titulo se ha descargado y funciona en local\n",
    "modelo = OllamaLLM(model =\"llama3.1\") \n",
    "# Query al modelo con el template\n",
    "prompt = ChatPromptTemplate.from_template(template) \n",
    "# Estos chains reflejan el flujo de interaccion entre el modelo y su entrada, en este caso el prompt del usuario que se inyecta en el modelo y se obtiene una respuesta\n",
    "chain = prompt | modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para generar el chat\n",
    "def chat():\n",
    "    \n",
    "    print(\"Lervis: Bienvenido a Lervis\")\n",
    "    context = \"\"\n",
    "    while True:\n",
    "        user_input = input(\"Usuario: \") # Input del usuario\n",
    "        print(f'Usuario: {user_input}')\n",
    "        if user_input == \"exit\": # Palabra clave para salir del chat\n",
    "            print('Un placer ayudarte, hasta pronto!')\n",
    "            break\n",
    "        result = chain.invoke({\"context\": context, \"question\": user_input, \"info_articles\": info_articles}) # Se utiliza el invoke para asi aniadir el contexto que va creciendo en cada iteracion\n",
    "        print(\"Lervis: \", result) # Se imprime la respuesta del modelo\n",
    "        context += f\"\\nUser: {user_input}\\nLervis: {result}\" # Actualizacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lervis: Bienvenido a Lervis\n",
      "Usuario: Quiero que me digas que publicaciones hay en el 2024.\n",
      "Lervis:  Hay una sola publicación del año 2024, que es \"The Future is Meta: Metadata, Formats and Perspectives towards Interactive and Personalized AV Content\" de Alexander Weller, Werner Bleisteiner, Christian Hufnagel y Michael Iber.\n",
      "Usuario: \n",
      "Lervis:  Hay solo una publicación del año 2024. Se trata de \"El futuro es meta: metadatos, formatos y perspectivas hacia contenido AV interactivo y personalizado\". Es un artículo escrito por Alexander Weller, Werner Bleisteiner, Christian Hufnagel y Michael Iber.\n",
      "Usuario: entiendo\n",
      "Lervis:  Entendido. ¿Necesitas algo más?\n",
      "Usuario: exit\n",
      "Un placer ayudarte, hasta pronto!\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
